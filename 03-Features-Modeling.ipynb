{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Main flavors of data and feature engineering\n",
    "* Tabular: Dataframe model\n",
    "    * \"Typical\" business data tables\n",
    "* Batch/Tensor/Vector: Array model\n",
    "    * Numeric data, timeseries, scientific data, audio, images, video, geodata, etc.\n",
    "* Natural language\n",
    "    * Batches of strings\n",
    "    * Transformed into array data through NLP-specific techniques\n",
    "    \n",
    "<img src='images/flow-transform.png' width=800>\n",
    "\n",
    "### \"Must-haves\" for feature engineering on large data\n",
    "\n",
    "* Some data representation for the large dataset\n",
    "    * Likely distributed, out-of-core, lazy, streaming, etc.\n",
    "* Mechanism to load data from standard formats and locations into the representation\n",
    "    * E.g., loading HDF5 in S3 or Parquet in HDFS\n",
    "* APIs to apply feature engineering transformations\n",
    "    * Mathematical operations\n",
    "    * String, date, etc.\n",
    "    * Custom (\"user-defined\")\n",
    "* Integration to a modeling framework and/or ability to write to standard formats\n",
    "\n",
    "### \"Nice-to-haves\"\n",
    "\n",
    "* Intuitive data representation: similar to \"small data\" tooling\n",
    "* APIs that resemble those of the most common industry-standard libraries\n",
    "* Both modeling integration *and* ability to write out transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/psf-logo@2x.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rise of Python\n",
    "\n",
    "Python has become the *lingua franca* or dominant cross-cutting language for data science.\n",
    "\n",
    ">\n",
    "> __Note__ this is not to imply Python is the best or only language, or that other languages might not be intrinsically better or even, in the future, more successful. \n",
    ">\n",
    "> There are wonderful things to be said for languages from Rust to R to Julia to many others, but for baseline data science capability and versatility in commercial enterprises today, it's Python\n",
    ">\n",
    "\n",
    "So we can turn to Python and look at the dominant libraries and tools within that ecosystem\n",
    "* Tabular data: Pandas\n",
    "* Array data: NumPy and derivatives like CuPy, JAX.numpy, etc.\n",
    "* Basic modeling: scikit-learn, XGBoost, etc.\n",
    "* Deep learning: PyTorch, Tensorflow\n",
    "* NLP: SpaCy, NLTK, Huggingface, etc.\n",
    "\n",
    "As we get into further parts of the workflow, like hyperparameter tuning or reinforcement learning there are more choices. \n",
    "\n",
    "For time reasons, we're going to stick to this core workflow of extraction through modeling and tuning, and not continue on into MLOps and deploment architectures, or meta-modeling platforms for experimentation, feature and provenance tracking, etc. That would be a bit too much to take on!\n",
    "\n",
    "__Bottom line__: We want a data representation and APIs that are fairly close to the Pandas / NumPy / scikit-learn (SciPy) workflow. And we want elegant bridges into things like PyTorch, XGBoost, NLP tools, and tuning tools.\n",
    "\n",
    "## Dask: SciPy at Scale\n",
    "\n",
    "Luckily, Dask is well placed to solve this problem. \n",
    "\n",
    "While enterprises were still wrestling with JVM-based tools over the past 5 years, scientists, researchers, and others in the PyData and SciPy communities were building Dask, a pure-Python distributed compute platform that integrates deeply with all of the standard SciPy tools.\n",
    "\n",
    "__What does this mean?__\n",
    "\n",
    "We can take many of our local workflows to large-scale data via Dask with fairly minimal effort -- because under the hood, Dask is designed to use those \"small data\" structures in federation to create arbitrarily large ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=left><tr><td>\n",
    "    <img src='images/dask-dataframe.svg' width=350>\n",
    "</td><td style='width:10em;'>\n",
    "</td><td>\n",
    "    <img src='images/dask-array.svg'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an added bonus, due to the Dask architecture, it can leverage GPU-enabled versions of the underlying libraries.\n",
    "* GPU + NumPy => CuPy\n",
    "* GPU + Pandas => cuDF (RAPIDS CUDA dataframe)\n",
    "* GPU + scikit-learn => cuML (RAPIDS CUDA algorithms)\n",
    "etc.\n",
    "\n",
    "### Using Dask for Feature Transformation\n",
    "\n",
    "* We need to be able to load data in a standard format\n",
    "* Manipulate it using dataframe or array APIs\n",
    "* Write it and/or pass it efficiently to a modeling framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as ddf\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=1, memory_limit='1GB')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.read_csv('data/diamonds.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.categorize()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = ddf.reshape.get_dummies(df)\n",
    "\n",
    "prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "<img src='images/flow-model.png' width=800/>\n",
    "\n",
    "If Dask makes an easy choice for some feature engineering and preprocessing, we're back into the deep end making choices for modeling.\n",
    "\n",
    "Why?\n",
    "\n",
    "Simply put, different kinds of modeling are handled best by different tools, so we have a lot of choices to make.\n",
    "\n",
    "* \"Classic\" ML\n",
    "    * Dask\n",
    "    * Dask ML\n",
    "    * XGBoost (with or without Dask)\n",
    "* Unsupervised learning and dimensionality reduction\n",
    "    * Dask supports some algorithms\n",
    "    * For others, we may want to scale a deep-learning tool (PyTorch/Tensorflow)\n",
    "        * Horovod\n",
    "        * Ray SGD\n",
    "* Deep learning (scaling PyTorch/TF easily)\n",
    "    * Horovod\n",
    "    * Ray SGD\n",
    "    * Ray RLlib for deep reinforcement learning\n",
    "* Simulations and agent-based models\n",
    "    * Ray for stateful-agent simulations\n",
    "    * Dask Actors may be an option\n",
    "\n",
    "\n",
    "## Example: Linear Model with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prepared.price.to_dask_array(lengths=True)\n",
    "arr = prepared.drop('price', axis=1).to_dask_array(lengths=True)\n",
    "\n",
    "arr[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:4].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arr, y, test_size=0.1)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(solver='lbfgs', max_iter=10)\n",
    "lr_model = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = lr_model.predict(X_test)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Ray?\n",
    "\n",
    "Ray (https://ray.io/) is a scale-out computing system designed for high-throughput, resilient stateful-actor algorithms. Ray was design at UC Berkeley's RISE lab under the supervision of some of the same team that created Apache Spark. \n",
    "\n",
    "Ray supports a number of languages at the API layer (Python and Java today) while most of the engine is C++. Ray's stateful actor support makes it strong in a number of key areas, like distributed SGD and reinforcement learning.\n",
    "\n",
    "Let's try a reinforcement learning example!\n",
    "\n",
    "> __Reinforcement Learning__ is a family of techniques that train *agents* to act in an *environment* to maximize *reward*. Famous examples include agents that can play chess, go, or Atari games ... but the field is hot because those agents can also be robots learning to do work, autonomous vehicles driving, or even virtual salesmen learning to get the best price possible from a customer.\n",
    "\n",
    "Ray treats deep reinforcement learning (RL + deep learning) as a top-level use case and includes libraries that encapsulate many of the most popular algorithms.\n",
    "\n",
    "Here, to create a simple example, we'll use __Deep Q-Learning__ (a foundational deep RL algorithm) to learn OpenAI's \"cart-pole\" (https://gym.openai.com/envs/CartPole-v1/) environment, which you can visualize like this:\n",
    "\n",
    "<video src='images/cpv1.mp4' controls=\"true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example, and the lab, are based on the demo in Dean Wampler's excellent intro paper, \"What is Ray?\" on O'Reilly Safari Online: https://learning.oreilly.com/library/view/what-is-ray/9781492085768/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies the OpenAI Gym environment for CartPole, V1.\n",
    "SELECT_ENV = \"CartPole-v1\"\n",
    "\n",
    "# Number of training runs.\n",
    "N_ITER = 50\n",
    "\n",
    "# default configuration.\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "\n",
    "# Suppress too many messages.\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "# Use > 1 for more CPU cores, e.g., over a cluster.\n",
    "config['num_workers'] = 2\n",
    "\n",
    "# Describe network\n",
    "config['model']['fcnet_hiddens'] = [40,20]\n",
    "\n",
    "# Don't pin a CPU core to each worker (allows more workers).\n",
    "config['num_cpus_per_worker'] = 0\n",
    "checkpoint_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dqn.DQNTrainer(config, SELECT_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '{:3d},{:8.4f},{:8.4f},{:8.4f}'\n",
    "last_checkpoint = ''\n",
    "for n in range(N_ITER):\n",
    "    result = trainer.train()\n",
    "    min  = result['episode_reward_min']\n",
    "    mean = result['episode_reward_mean']\n",
    "    max  = result['episode_reward_max']\n",
    "    last_checkpoint = trainer.save(checkpoint_dir)\n",
    "    print(fmt.format(n, min, mean, max))\n",
    "print(f'last checkpoint file: {last_checkpoint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you've worked with RL and OpenAI gym before, you may realize these are not particularly impressive numbers, and not a particularly impressive algorithm.\n",
    "\n",
    "Don't worry: __Ray RLlib__ includes a variety of much more powerful algorithms which achieve better results. We'll try one of them -- Proximal Policy Optimization (PPO) in the lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
