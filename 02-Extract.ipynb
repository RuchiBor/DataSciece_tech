{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring data (extraction)\n",
    "\n",
    "<img src='images/flow-extract.png' width=800>\n",
    "\n",
    "> Note: in some organizations, there is a data discovery system, like https://www.amundsen.io/amundsen/ upstream from this step. We're not covering that area due to scope constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: use SQL to efficiently retrieve data for further work\n",
    "\n",
    "### Legacy Tools\n",
    "\n",
    "Mostly: Apache Hive\n",
    "\n",
    "### Current Tools\n",
    "\n",
    "* SparkSQL\n",
    "* Presto\n",
    "* *Hive Metastore*\n",
    "\n",
    "### Rising/Future Tools\n",
    "\n",
    "* Kartothek, Intake\n",
    "* BlazingSQL\n",
    "* Dask-SQL\n",
    "\n",
    "*There are more non-SQL options, but support for SQL is a requirement in most large organizations, so we're sticking with SQL-capable tools for now*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`data/california`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT origin, mean(delay) as delay, count(1) \n",
    "FROM parquet.`data/california` \n",
    "GROUP BY origin\n",
    "HAVING count(1) > 500\n",
    "ORDER BY delay DESC\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM parquet.`data/california` \n",
    "WHERE origin in (\n",
    "    SELECT origin \n",
    "    FROM parquet.`data/california` \n",
    "    GROUP BY origin \n",
    "    HAVING count(1) > 500\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(query).write.mode('overwrite').option('header', 'true').csv('data/refined_flights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/refined_flights/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
